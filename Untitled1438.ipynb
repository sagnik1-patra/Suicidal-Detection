{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1e42f3-bbec-4c76-8bc2-fd5cf8746a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MindShield Artifacts Written ===\n",
      "H5:    C:\\Users\\sagni\\Downloads\\Suicidal Detection\\mindshield_dataset.h5\n",
      "PKL:   C:\\Users\\sagni\\Downloads\\Suicidal Detection\\mindshield_dataset.pkl\n",
      "YAML:  C:\\Users\\sagni\\Downloads\\Suicidal Detection\\mindshield_config.yaml\n",
      "JSONL: C:\\Users\\sagni\\Downloads\\Suicidal Detection\\mindshield_dataset.jsonl\n",
      "SUM:   C:\\Users\\sagni\\Downloads\\Suicidal Detection\\mindshield_summary.json\n",
      "\n",
      "Sizes: {'train': 8858, 'valid': 1898, 'test': 1899}\n",
      "Class balance: {'train': {'1': 4625, '0': 4233}, 'valid': {'1': 991, '0': 907}, 'test': {'1': 992, '0': 907}}\n",
      "Label mapping: {0: 'non-suicidal', 1: 'suicidal'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "    HAVE_YAML = True\n",
    "except Exception:\n",
    "    HAVE_YAML = False\n",
    "\n",
    "\n",
    "# --------- USER PATHS (edit if needed) ----------\n",
    "CSV_PATH = r\"C:\\Users\\sagni\\Downloads\\Suicidal Detection\\archive\\suicidal_ideation_reddit_annotated.csv\"\n",
    "OUT_DIR  = r\"C:\\Users\\sagni\\Downloads\\Suicidal Detection\"\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "# Common name candidates\n",
    "TEXT_CANDIDATES = [\n",
    "    \"text\", \"message\", \"content\", \"body\", \"post\", \"comment\", \"clean_text\", \"utterance\",\n",
    "    \"selftext\", \"title\"\n",
    "]\n",
    "LABEL_CANDIDATES = [\n",
    "    \"label\", \"class\", \"target\", \"is_suicidal\", \"suicidal\", \"suicide\", \"risk\", \"y\"\n",
    "]\n",
    "\n",
    "# String->binary mapping heuristics\n",
    "POS_TOKENS = {\"1\",\"true\",\"t\",\"yes\",\"y\",\"suicidal\",\"suicide\",\"positive\",\"pos\",\"high\",\"at risk\"}\n",
    "NEG_TOKENS = {\"0\",\"false\",\"f\",\"no\",\"n\",\"non-suicidal\",\"non suicidal\",\"negative\",\"neg\",\"low\",\"not at risk\"}\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "SPLIT_FRAC = (0.70, 0.15, 0.15)  # train, valid, test\n",
    "\n",
    "\n",
    "def ensure_out_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_csv(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"CSV not found: {path}\")\n",
    "    # Try utf-8 first; fallback to latin-1\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, encoding=\"latin-1\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def detect_text_and_label(df: pd.DataFrame) -> Tuple[pd.Series, pd.Series, str, str]:\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    # Text detection:\n",
    "    text_col: Optional[str] = None\n",
    "    for cand in TEXT_CANDIDATES:\n",
    "        if cand in cols_lower:\n",
    "            text_col = cols_lower[cand]\n",
    "            break\n",
    "\n",
    "    # Special Reddit-style: combine title + selftext if both exist and no single text found\n",
    "    if text_col is None and \"title\" in cols_lower and \"selftext\" in cols_lower:\n",
    "        title_col = cols_lower[\"title\"]\n",
    "        st_col = cols_lower[\"selftext\"]\n",
    "        text = (df[title_col].fillna(\"\").astype(str) + \" \" + df[st_col].fillna(\"\").astype(str)).str.strip()\n",
    "    else:\n",
    "        if text_col is None:\n",
    "            # fallback: use the first object/string dtype column\n",
    "            obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "            if not obj_cols:\n",
    "                raise ValueError(\"Could not detect a text column. Please rename your text column to 'text'.\")\n",
    "            text_col = obj_cols[0]\n",
    "        text = df[text_col].astype(str)\n",
    "\n",
    "    # Clean empties\n",
    "    text = text.fillna(\"\").astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    text = text[text != \"\"]\n",
    "    df = df.loc[text.index]  # align\n",
    "\n",
    "    # Label detection:\n",
    "    label_col: Optional[str] = None\n",
    "    for cand in LABEL_CANDIDATES:\n",
    "        if cand in cols_lower:\n",
    "            label_col = cols_lower[cand]\n",
    "            break\n",
    "    if label_col is None:\n",
    "        # Try to infer from columns that look binary (0/1 or True/False)\n",
    "        for c in df.columns:\n",
    "            ser = df[c]\n",
    "            unique = pd.Series(ser.dropna().unique()).astype(str).str.lower().str.strip()\n",
    "            if len(unique) <= 6 and unique.isin(list(POS_TOKENS | NEG_TOKENS)).any():\n",
    "                label_col = c\n",
    "                break\n",
    "\n",
    "    if label_col is None:\n",
    "        raise ValueError(\n",
    "            f\"Could not detect label column. \"\n",
    "            f\"Please rename your label column to one of {LABEL_CANDIDATES}.\"\n",
    "        )\n",
    "\n",
    "    labels = normalize_labels(df[label_col])\n",
    "    # Align with cleaned text index\n",
    "    labels = labels.loc[text.index]\n",
    "    return text, labels, text_col if text_col else \"title+selftext\", label_col\n",
    "\n",
    "\n",
    "def normalize_labels(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Map label values to {0,1}, robust to text variants.\"\"\"\n",
    "    s = series.copy()\n",
    "\n",
    "    def to01(x: Any) -> Optional[int]:\n",
    "        if pd.isna(x):\n",
    "            return None\n",
    "        if isinstance(x, (int, np.integer, float, np.floating)):\n",
    "            if int(x) in (0,1): return int(x)\n",
    "        xs = str(x).strip().lower()\n",
    "        if xs in POS_TOKENS: return 1\n",
    "        if xs in NEG_TOKENS: return 0\n",
    "        # common words\n",
    "        if re.fullmatch(r\"(non[-\\s]?suicidal|no risk|not suicidal)\", xs): return 0\n",
    "        if re.fullmatch(r\"(suicidal|at[-\\s]?risk|high risk|suicide)\", xs): return 1\n",
    "        # last resort: try integer cast\n",
    "        try:\n",
    "            vi = int(float(xs))\n",
    "            if vi in (0,1): return vi\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    mapped = s.map(to01)\n",
    "    # Drop rows that can't be mapped\n",
    "    mask = mapped.isin([0,1])\n",
    "    if mask.sum() == 0:\n",
    "        raise ValueError(\"Could not map labels to {0,1}. Please ensure labels indicate suicidal vs non.\")\n",
    "    return mapped[mask].astype(int)\n",
    "\n",
    "\n",
    "def make_splits(text: pd.Series, labels: pd.Series) -> Dict[str, pd.DataFrame]:\n",
    "    df = pd.DataFrame({\"text\": text, \"label\": labels}).dropna()\n",
    "    # First split: train vs temp\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, test_size=(1.0 - SPLIT_FRAC[0]),\n",
    "        stratify=df[\"label\"], random_state=RANDOM_STATE, shuffle=True\n",
    "    )\n",
    "    # Second split: valid/test from temp\n",
    "    valid_size = SPLIT_FRAC[1] / (SPLIT_FRAC[1] + SPLIT_FRAC[2])\n",
    "    valid_df, test_df = train_test_split(\n",
    "        temp_df, test_size=(1.0 - valid_size),\n",
    "        stratify=temp_df[\"label\"], random_state=RANDOM_STATE, shuffle=True\n",
    "    )\n",
    "    for split, d in [(\"train\", train_df), (\"valid\", valid_df), (\"test\", test_df)]:\n",
    "        d.reset_index(drop=True, inplace=True)\n",
    "    return {\"train\": train_df, \"valid\": valid_df, \"test\": test_df}\n",
    "\n",
    "\n",
    "def write_h5(path: str, splits: Dict[str, pd.DataFrame]) -> None:\n",
    "    \"\"\"Write HDF5 without PyTables (uses h5py).\"\"\"\n",
    "    with h5py.File(path, \"w\") as h5:\n",
    "        for split, df in splits.items():\n",
    "            grp = h5.create_group(split)\n",
    "            # variable-length UTF-8 strings\n",
    "            str_dt = h5py.string_dtype(encoding=\"utf-8\")\n",
    "            texts = df[\"text\"].astype(str).values\n",
    "            labels = df[\"label\"].astype(np.int8).values\n",
    "            grp.create_dataset(\"text\", data=texts, dtype=str_dt, compression=\"gzip\")\n",
    "            grp.create_dataset(\"label\", data=labels, dtype=np.int8, compression=\"gzip\")\n",
    "\n",
    "\n",
    "def write_pkl(path: str, splits: Dict[str, pd.DataFrame], meta: Dict[str, Any]) -> None:\n",
    "    payload = {\n",
    "        \"splits\": {k: v.copy() for k, v in splits.items()},\n",
    "        \"meta\": meta\n",
    "    }\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def write_yaml(path: str, meta: Dict[str, Any]) -> None:\n",
    "    if HAVE_YAML:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            yaml.safe_dump(meta, f, sort_keys=False, allow_unicode=True)\n",
    "    else:\n",
    "        # Fallback: write JSON but with .yaml extension (so you get *something*)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(meta, ensure_ascii=False, indent=2))\n",
    "\n",
    "\n",
    "def write_jsonl(path: str, splits: Dict[str, pd.DataFrame]) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for split, df in splits.items():\n",
    "            for _, row in df.iterrows():\n",
    "                rec = {\"split\": split, \"text\": row[\"text\"], \"label\": int(row[\"label\"])}\n",
    "                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def class_counts(df: pd.DataFrame) -> Dict[str, int]:\n",
    "    c = df[\"label\"].value_counts().to_dict()\n",
    "    return {str(int(k)): int(v) for k, v in c.items()}\n",
    "\n",
    "\n",
    "def main():\n",
    "    ensure_out_dir(OUT_DIR)\n",
    "    df = load_csv(CSV_PATH)\n",
    "    text, labels, text_col_name, label_col_name = detect_text_and_label(df)\n",
    "\n",
    "    # Align indices after normalization/cleaning\n",
    "    aligned = pd.DataFrame({\"text\": text, \"label\": labels}).dropna()\n",
    "    splits = make_splits(aligned[\"text\"], aligned[\"label\"])\n",
    "\n",
    "    # Build metadata\n",
    "    meta = {\n",
    "        \"dataset_name\": \"suicidal_ideation_reddit_annotated\",\n",
    "        \"source_csv\": CSV_PATH,\n",
    "        \"created_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"text_column_used\": text_col_name,\n",
    "        \"label_column_used\": label_col_name,\n",
    "        \"label_mapping\": {0: \"non-suicidal\", 1: \"suicidal\"},\n",
    "        \"sizes\": {k: int(v.shape[0]) for k, v in splits.items()},\n",
    "        \"class_balance\": {\n",
    "            split: class_counts(df_split) for split, df_split in splits.items()\n",
    "        },\n",
    "        \"splits\": {\"train\": SPLIT_FRAC[0], \"valid\": SPLIT_FRAC[1], \"test\": SPLIT_FRAC[2]},\n",
    "        \"random_state\": RANDOM_STATE\n",
    "    }\n",
    "\n",
    "    # File paths\n",
    "    out_h5   = os.path.join(OUT_DIR, \"mindshield_dataset.h5\")\n",
    "    out_pkl  = os.path.join(OUT_DIR, \"mindshield_dataset.pkl\")\n",
    "    out_yaml = os.path.join(OUT_DIR, \"mindshield_config.yaml\")\n",
    "    out_jsonl= os.path.join(OUT_DIR, \"mindshield_dataset.jsonl\")\n",
    "    out_sum  = os.path.join(OUT_DIR, \"mindshield_summary.json\")\n",
    "\n",
    "    # Write artifacts\n",
    "    write_h5(out_h5, splits)\n",
    "    write_pkl(out_pkl, splits, meta)\n",
    "    write_yaml(out_yaml, meta)\n",
    "    write_jsonl(out_jsonl, splits)\n",
    "    with open(out_sum, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Console summary\n",
    "    print(\"=== MindShield Artifacts Written ===\")\n",
    "    print(f\"H5:    {out_h5}\")\n",
    "    print(f\"PKL:   {out_pkl}\")\n",
    "    print(f\"YAML:  {out_yaml}\")\n",
    "    print(f\"JSONL: {out_jsonl}\")\n",
    "    print(f\"SUM:   {out_sum}\")\n",
    "    print(\"\\nSizes:\", meta[\"sizes\"])\n",
    "    print(\"Class balance:\", meta[\"class_balance\"])\n",
    "    print(\"Label mapping:\", meta[\"label_mapping\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9e01d-9c92-4e3a-aa13-4555cdb36e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
