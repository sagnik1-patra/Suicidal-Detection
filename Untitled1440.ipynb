{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f33f187-2807-4276-9088-5daab0f05c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Saved model to: C:\\Users\\sagni\\Downloads\\Suicidal Detection\\ms_model.joblib\n",
      "[TRAIN] Test metrics: {'accuracy': 0.8662, 'f1': 0.8713, 'precision': 0.8758, 'recall': 0.8669}\n",
      "[INFER] Saved predictions:\n",
      "  CSV : C:\\Users\\sagni\\Downloads\\Suicidal Detection\\ms_predictions.csv\n",
      "  JSON: C:\\Users\\sagni\\Downloads\\Suicidal Detection\\ms_predictions.json\n",
      "[TEST] Metrics: {'accuracy': 0.866245392311743, 'f1': 0.8713272543059777, 'precision': 0.8757637474541752, 'recall': 0.8669354838709677}\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, warnings\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional, Tuple, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ------------------- USER PATHS -------------------\n",
    "CSV_PATH   = r\"C:\\Users\\sagni\\Downloads\\Suicidal Detection\\archive\\suicidal_ideation_reddit_annotated.csv\"\n",
    "OUT_DIR    = r\"C:\\Users\\sagni\\Downloads\\Suicidal Detection\"\n",
    "MODEL_PATH = os.path.join(OUT_DIR, \"ms_model.joblib\")\n",
    "\n",
    "# If None, we evaluate on the TEST split created from the training CSV.\n",
    "# Otherwise, set to a path of .txt / .csv / .json to run predictions on that file.\n",
    "INFER_SOURCE = None\n",
    "# Examples:\n",
    "# INFER_SOURCE = r\"C:\\Users\\sagni\\Downloads\\Suicidal Detection\\some_new_texts.txt\"\n",
    "# INFER_SOURCE = r\"C:\\Users\\sagni\\Downloads\\Suicidal Detection\\new_messages.csv\"\n",
    "# INFER_SOURCE = r\"C:\\Users\\sagni\\Downloads\\Suicidal Detection\\messages.json\"\n",
    "# --------------------------------------------------\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "SPLIT_FRAC = (0.70, 0.15, 0.15)  # train/valid/test\n",
    "\n",
    "TEXT_CANDS = [\n",
    "    \"text\", \"message\", \"content\", \"body\", \"post\", \"comment\", \"clean_text\", \"utterance\",\n",
    "    \"selftext\", \"title\"\n",
    "]\n",
    "LABEL_CANDS = [\"label\", \"class\", \"target\", \"is_suicidal\", \"suicidal\", \"suicide\", \"risk\", \"y\"]\n",
    "\n",
    "POS_TOKENS = {\"1\",\"true\",\"t\",\"yes\",\"y\",\"suicidal\",\"suicide\",\"positive\",\"pos\",\"high\",\"at risk\"}\n",
    "NEG_TOKENS = {\"0\",\"false\",\"f\",\"no\",\"n\",\"non-suicidal\",\"non suicidal\",\"negative\",\"neg\",\"low\",\"not at risk\"}\n",
    "\n",
    "# ------------------- UTILITIES -------------------\n",
    "def ensure_out():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_csv(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"latin-1\")\n",
    "\n",
    "def normalize_labels(series: pd.Series) -> pd.Series:\n",
    "    def to01(x: Any) -> Optional[int]:\n",
    "        if pd.isna(x): return None\n",
    "        if isinstance(x, (int, np.integer, float, np.floating)):\n",
    "            xi = int(x)\n",
    "            if xi in (0,1): return xi\n",
    "        xs = str(x).strip().lower()\n",
    "        if xs in POS_TOKENS: return 1\n",
    "        if xs in NEG_TOKENS: return 0\n",
    "        if re.fullmatch(r\"(non[-\\s]?suicidal|no risk|not suicidal)\", xs): return 0\n",
    "        if re.fullmatch(r\"(suicidal|at[-\\s]?risk|high risk|suicide)\", xs): return 1\n",
    "        try:\n",
    "            vi = int(float(xs))\n",
    "            if vi in (0,1): return vi\n",
    "        except: pass\n",
    "        return None\n",
    "\n",
    "    mapped = series.map(to01)\n",
    "    mask = mapped.isin([0,1])\n",
    "    if mask.sum() == 0:\n",
    "        raise ValueError(\"Could not map labels to {0,1}. Ensure labels indicate suicidal vs non.\")\n",
    "    return mapped[mask].astype(int)\n",
    "\n",
    "def detect_train_columns(df: pd.DataFrame) -> Tuple[pd.Series, pd.Series, str, str]:\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    # Text\n",
    "    text_col = None\n",
    "    for c in TEXT_CANDS:\n",
    "        if c in cols_lower:\n",
    "            text_col = cols_lower[c]\n",
    "            break\n",
    "\n",
    "    if text_col is None and \"title\" in cols_lower and \"selftext\" in cols_lower:\n",
    "        title_col = cols_lower[\"title\"]\n",
    "        st_col = cols_lower[\"selftext\"]\n",
    "        text = (df[title_col].fillna(\"\").astype(str) + \" \" + df[st_col].fillna(\"\").astype(str)).str.strip()\n",
    "    else:\n",
    "        if text_col is None:\n",
    "            obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "            if not obj_cols:\n",
    "                raise ValueError(\"No obvious text column. Rename to 'text'.\")\n",
    "            text_col = obj_cols[0]\n",
    "        text = df[text_col].astype(str)\n",
    "\n",
    "    text = text.fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    text = text[text != \"\"]\n",
    "    df = df.loc[text.index]\n",
    "\n",
    "    # Label\n",
    "    label_col = None\n",
    "    for c in LABEL_CANDS:\n",
    "        if c in cols_lower:\n",
    "            label_col = cols_lower[c]\n",
    "            break\n",
    "    if label_col is None:\n",
    "        for c in df.columns:\n",
    "            uniq = pd.Series(df[c].dropna().unique()).astype(str).str.lower().str.strip()\n",
    "            if len(uniq) <= 6 and uniq.isin(list(POS_TOKENS | NEG_TOKENS)).any():\n",
    "                label_col = c; break\n",
    "    if label_col is None:\n",
    "        raise ValueError(\"No label column found. Rename to 'label' or add to LABEL_CANDS.\")\n",
    "\n",
    "    labels = normalize_labels(df[label_col])\n",
    "    labels = labels.loc[text.index]\n",
    "    return text, labels, (text_col if text_col else \"title+selftext\"), label_col\n",
    "\n",
    "def make_splits(text: pd.Series, y: pd.Series):\n",
    "    df = pd.DataFrame({\"text\": text, \"label\": y}).dropna()\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, test_size=(1.0 - SPLIT_FRAC[0]),\n",
    "        stratify=df[\"label\"], random_state=RANDOM_STATE, shuffle=True\n",
    "    )\n",
    "    valid_size = SPLIT_FRAC[1] / (SPLIT_FRAC[1] + SPLIT_FRAC[2])\n",
    "    valid_df, test_df = train_test_split(\n",
    "        temp_df, test_size=(1.0 - valid_size),\n",
    "        stratify=temp_df[\"label\"], random_state=RANDOM_STATE, shuffle=True\n",
    "    )\n",
    "    for d in (train_df, valid_df, test_df):\n",
    "        d.reset_index(drop=True, inplace=True)\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "def build_pipeline() -> Pipeline:\n",
    "    return Pipeline(steps=[\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            lowercase=True,\n",
    "            ngram_range=(1,2),       # unigrams + bigrams\n",
    "            min_df=2,                # ignore super-rare noise\n",
    "            max_features=200000,     # cap features\n",
    "            strip_accents=\"unicode\"\n",
    "        )),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            solver=\"saga\",\n",
    "            penalty=\"l2\",\n",
    "            C=1.0,\n",
    "            class_weight=\"balanced\",  # handle class imbalance\n",
    "            max_iter=300,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# ------------------- TRAIN + SAVE -------------------\n",
    "def train_and_save_model() -> Dict[str, Any]:\n",
    "    df = load_csv(CSV_PATH)\n",
    "    text, y, text_col, label_col = detect_train_columns(df)\n",
    "    train_df, valid_df, test_df = make_splits(text, y)\n",
    "\n",
    "    X_train = np.concatenate([train_df[\"text\"].values, valid_df[\"text\"].values])\n",
    "    y_train = np.concatenate([train_df[\"label\"].values, valid_df[\"label\"].values])\n",
    "\n",
    "    pipe = build_pipeline()\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    dump(pipe, MODEL_PATH)\n",
    "\n",
    "    # quick test metrics for reference\n",
    "    y_pred = pipe.predict(test_df[\"text\"].values)\n",
    "    acc  = float(accuracy_score(test_df[\"label\"].values, y_pred))\n",
    "    f1   = float(f1_score(test_df[\"label\"].values, y_pred, zero_division=0))\n",
    "    prec = float(precision_score(test_df[\"label\"].values, y_pred, zero_division=0))\n",
    "    rec  = float(recall_score(test_df[\"label\"].values, y_pred, zero_division=0))\n",
    "\n",
    "    # Save a report\n",
    "    rep = classification_report(\n",
    "        test_df[\"label\"].values, y_pred,\n",
    "        target_names=[\"Non-suicidal (0)\", \"Suicidal (1)\"], zero_division=0\n",
    "    )\n",
    "    with open(os.path.join(OUT_DIR, \"ms_classification_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(rep)\n",
    "\n",
    "    summary = {\n",
    "        \"created_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"source_csv\": CSV_PATH,\n",
    "        \"text_column_used\": text_col,\n",
    "        \"label_column_used\": label_col,\n",
    "        \"splits_sizes\": {\n",
    "            \"train\": int(len(train_df)), \"valid\": int(len(valid_df)), \"test\": int(len(test_df))\n",
    "        },\n",
    "        \"test_metrics\": {\n",
    "            \"accuracy\": round(acc, 4),\n",
    "            \"f1\": round(f1, 4),\n",
    "            \"precision\": round(prec, 4),\n",
    "            \"recall\": round(rec, 4)\n",
    "        },\n",
    "        \"model_path\": MODEL_PATH\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, \"ms_infer_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"[TRAIN] Saved model to:\", MODEL_PATH)\n",
    "    print(\"[TRAIN] Test metrics:\", summary[\"test_metrics\"])\n",
    "    return {\"pipe\": pipe, \"test_df\": test_df, \"summary\": summary}\n",
    "\n",
    "# ------------------- INFERENCE HELPERS -------------------\n",
    "def load_model() -> Pipeline:\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Model not found at {MODEL_PATH}. Run training first.\")\n",
    "    return load(MODEL_PATH)\n",
    "\n",
    "def _infer_from_texts(pipe: Pipeline, texts: List[str]) -> pd.DataFrame:\n",
    "    preds = pipe.predict(texts)\n",
    "    if hasattr(pipe[-1], \"predict_proba\"):\n",
    "        prob1 = pipe.predict_proba(texts)[:, 1]\n",
    "    else:\n",
    "        prob1 = np.full(len(preds), np.nan, dtype=float)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"text\": texts,\n",
    "        \"pred_label\": preds.astype(int),\n",
    "        \"prob_suicidal\": prob1\n",
    "    })\n",
    "    df[\"pred_name\"] = np.where(df[\"pred_label\"] == 1, \"suicidal\", \"non-suicidal\")\n",
    "    return df\n",
    "\n",
    "def infer_from_file(pipe: Pipeline, path: str) -> pd.DataFrame:\n",
    "    p = str(path)\n",
    "    ext = os.path.splitext(p)[1].lower()\n",
    "    if ext == \".txt\":\n",
    "        with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            lines = [ln.strip() for ln in f if ln.strip()]\n",
    "        return _infer_from_texts(pipe, lines)\n",
    "    elif ext == \".json\":\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # accept {\"messages\":[{\"text\": ...}, ...]} or list[str]/list[dict{text:...}]\n",
    "        if isinstance(data, dict) and \"messages\" in data:\n",
    "            msgs = data[\"messages\"]\n",
    "            texts = [m[\"text\"] if isinstance(m, dict) and \"text\" in m else str(m) for m in msgs]\n",
    "            return _infer_from_texts(pipe, texts)\n",
    "        if isinstance(data, list):\n",
    "            texts = [d[\"text\"] if isinstance(d, dict) and \"text\" in d else str(d) for d in data]\n",
    "            return _infer_from_texts(pipe, texts)\n",
    "        raise ValueError(\"Unsupported JSON schema for inference.\")\n",
    "    elif ext == \".csv\":\n",
    "        df = pd.read_csv(p)\n",
    "        # detect a text column\n",
    "        text_col = None\n",
    "        lower = {c.lower(): c for c in df.columns}\n",
    "        for c in TEXT_CANDS:\n",
    "            if c in lower: text_col = lower[c]; break\n",
    "        if text_col is None:\n",
    "            # Single-column CSV fallback\n",
    "            if df.shape[1] == 1:\n",
    "                text_col = df.columns[0]\n",
    "            else:\n",
    "                raise ValueError(\"CSV must contain a text column (e.g., 'text').\")\n",
    "        texts = df[text_col].astype(str).fillna(\"\").str.strip().tolist()\n",
    "        texts = [t for t in texts if t]\n",
    "        return _infer_from_texts(pipe, texts)\n",
    "    else:\n",
    "        raise ValueError(\"Supported inference files: .txt, .csv, .json\")\n",
    "\n",
    "def save_predictions(df: pd.DataFrame, base_name: str = \"ms_predictions\"):\n",
    "    csv_path  = os.path.join(OUT_DIR, f\"{base_name}.csv\")\n",
    "    json_path = os.path.join(OUT_DIR, f\"{base_name}.json\")\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        payload = {\n",
    "            \"created_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"items\": df.to_dict(orient=\"records\"),\n",
    "            \"summary\": {\n",
    "                \"total\": int(len(df)),\n",
    "                \"pred_counts\": {\n",
    "                    \"non-suicidal\": int((df[\"pred_label\"]==0).sum()),\n",
    "                    \"suicidal\": int((df[\"pred_label\"]==1).sum())\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    print(\"[INFER] Saved predictions:\")\n",
    "    print(\"  CSV :\", csv_path)\n",
    "    print(\"  JSON:\", json_path)\n",
    "\n",
    "# ------------------- MAIN FLOW -------------------\n",
    "def main():\n",
    "    ensure_out()\n",
    "\n",
    "    # Train model if missing; else load\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        train_out = train_and_save_model()\n",
    "        pipe = train_out[\"pipe\"]\n",
    "        test_df = train_out[\"test_df\"]\n",
    "    else:\n",
    "        print(\"[INFO] Loading existing model:\", MODEL_PATH)\n",
    "        pipe = load_model()\n",
    "        # still need test_df for evaluation if INFER_SOURCE is None\n",
    "        df = load_csv(CSV_PATH)\n",
    "        text, y, *_ = detect_train_columns(df)\n",
    "        _, _, test_df = make_splits(text, y)\n",
    "\n",
    "    if INFER_SOURCE is None:\n",
    "        # Evaluate on TEST split and save predictions\n",
    "        infer_df = _infer_from_texts(pipe, test_df[\"text\"].tolist())\n",
    "        infer_df.insert(1, \"y_true\", test_df[\"label\"].values)\n",
    "\n",
    "        acc  = float(accuracy_score(test_df[\"label\"].values, infer_df[\"pred_label\"].values))\n",
    "        f1   = float(f1_score(test_df[\"label\"].values, infer_df[\"pred_label\"].values, zero_division=0))\n",
    "        prec = float(precision_score(test_df[\"label\"].values, infer_df[\"pred_label\"].values, zero_division=0))\n",
    "        rec  = float(recall_score(test_df[\"label\"].values, infer_df[\"pred_label\"].values, zero_division=0))\n",
    "\n",
    "        # Append metrics to summary file\n",
    "        summary_path = os.path.join(OUT_DIR, \"ms_infer_summary.json\")\n",
    "        base_summary = {}\n",
    "        if os.path.exists(summary_path):\n",
    "            try:\n",
    "                with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    base_summary = json.load(f)\n",
    "            except Exception:\n",
    "                base_summary = {}\n",
    "        base_summary.setdefault(\"inference_runs\", [])\n",
    "        base_summary[\"inference_runs\"].append({\n",
    "            \"run_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"source\": \"TEST_SPLIT\",\n",
    "            \"count\": int(len(infer_df)),\n",
    "            \"metrics\": {\n",
    "                \"accuracy\": round(acc, 4), \"f1\": round(f1, 4),\n",
    "                \"precision\": round(prec, 4), \"recall\": round(rec, 4)\n",
    "            }\n",
    "        })\n",
    "        with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(base_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        save_predictions(infer_df, base_name=\"ms_predictions\")\n",
    "        print(\"[TEST] Metrics:\", {\"accuracy\": acc, \"f1\": f1, \"precision\": prec, \"recall\": rec})\n",
    "    else:\n",
    "        # Predict on external file\n",
    "        infer_df = infer_from_file(pipe, INFER_SOURCE)\n",
    "        save_predictions(infer_df, base_name=\"ms_predictions_external\")\n",
    "        print(f\"[INFER] Ran predictions on: {INFER_SOURCE}\")\n",
    "        print(infer_df.head(10))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc6398-ecac-4f37-a44e-4b804749adca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
